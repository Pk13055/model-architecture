{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "First we need to load the data from the pickle objects and perform different preprocessing techniques.\n",
    "\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    \n",
    "    trn_data, trn_labels, tst_data, tst_labels = [], [], [], []\n",
    "    unpickle = lambda file: pickle.load(open(file, 'rb'), encoding='latin1')\n",
    "    for i in range(5):\n",
    "        batchName = f'./data/cifar-10-batches-py/data_batch_{i + 1}'\n",
    "        unpickled = unpickle(batchName)\n",
    "        trn_data.extend(unpickled['data'])\n",
    "        trn_labels.extend(unpickled['labels'])\n",
    "    unpickled = unpickle('./data/cifar-10-batches-py/test_batch')\n",
    "    tst_data.extend(unpickled['data'])\n",
    "    tst_labels.extend(unpickled['labels'])\n",
    "    return trn_data, trn_labels, tst_data, tst_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "After loading the data, we can use the function below to apply different preprocessing techniques to allow for better training and feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prep(X, y, onehot=True):\n",
    "    ''' pre-processes the given image\n",
    "        performs mean normalization and other such operations'''\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    X_ = scaler.fit_transform(X)\n",
    "    y_ = y\n",
    "    onehot = None\n",
    "    if onehot:\n",
    "        onehot = OneHotEncoder()\n",
    "        y_ = onehot.fit_transform(y)\n",
    "    return X_, y_, onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "There is an option to train on the complete set of **raw** images but this can lead to overfitting. Instead we can use PCA or LDA to extract and keep **only** those dimensions that best represent the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(X, y, **kwargs):\n",
    "    ''' performs dimensionality reduction'''\n",
    "    method = kwargs.pop('method', None)\n",
    "    if method in ['pca', 'lda']:\n",
    "        n_components = kwargs.pop('n_components', None)\n",
    "        if method == 'pca':\n",
    "            reducer = PCA(n_components=n_components)\n",
    "        elif method == 'lda':\n",
    "            reducer = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "        return reducer.fit_transform(X, y), reducer\n",
    "    elif method == 'raw':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Dimensionality reduction method not supported\")\n",
    "    return X, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "For classification, we prepare this model factory that takes in the type of decision maker and returns a model on which you can train. The various classification methods covered under this are:\n",
    "- SVM:\n",
    "    - Linear Kernel\n",
    "    - Gaussian Kernel\n",
    "    - Polynomial Kernel\n",
    "- Logistic Regression\n",
    "- MLP\n",
    "- Decision Tree\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, y, **kwargs):\n",
    "    ''' trains a classifier by taking input features\n",
    "        and their respective targets and returns the trained model'''\n",
    "    method = kwargs.pop('method')\n",
    "    options = kwargs.pop('options', {})\n",
    "    if method == 'SVM':\n",
    "        clf = svm.SVC(kernel='linear', **options)\n",
    "    elif method == 'RBF':\n",
    "        clf = svm.SVC(kernel='rbf')\n",
    "    elif method == 'logistic':\n",
    "        clf = LogisticRegression(**options)\n",
    "    elif method == 'MLP':\n",
    "        clf = MLPClassifier(**options)\n",
    "    elif method == 'CART':\n",
    "        clf = DecisionTreeClassifier(**options)\n",
    "    elif method == 'grad_boost':\n",
    "        clf = GradientBoostingClassifier(**options)\n",
    "    else:\n",
    "        print(\"Classifier not supported\")\n",
    "        return None, None\n",
    "    clf.fit(X, y)\n",
    "    return clf, method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Now that we have all the tools ready for the testing of various methods of classification, we can quantitavely run each classification method with our choice of preprocessing and evaluate the results. Our evaluation metric of choice is the accuracy and F1 score. The F1 score can be represented as follows:\n",
    "$$ F_1 = 2 * \\frac{ \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(train_X, train_y, test_X, test_y, meta):\n",
    "    '''takes test data and trained classifier model,\n",
    "    performs classification and prints accuracy and f1-score'''\n",
    "    model_kwargs, processing_kwargs = meta['model'], meta['preprocess']\n",
    "    train_X, reducer = reduce_dim(train_X, train_y, **processing_kwargs)\n",
    "    model_name, preprocessor = model_kwargs['method'], processing_kwargs['method']\n",
    "    print(f\"Model {model_name} | Preprocessing {preprocessor} started\")\n",
    "    model = classify(train_X, train_y, **model_options)\n",
    "    test_X = reducer.transform(test_X)\n",
    "    predictions = model.predict(test_X)\n",
    "    accuracy = accuracy_score(test_y, predictions)\n",
    "    F1 = f1_score(test_y, predictions, average='micro')\n",
    "    print(f\"Model {model_kwargs} | Preprocessing {processing_kwargs} : {(accuracy, F1)}\")\n",
    "    return accuracy, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_X, train_y, test_X, test_y):\n",
    "    \"\"\"Evaluates the various models against params\n",
    "    :return result: np.array -> The accuracy and F1 score for various training options\n",
    "    \"\"\"\n",
    "    results, metadata = [], []\n",
    "    classifiers = [\"SVM\", \"RBF\", \"logistic\", \"MLP\", \"CART\", 'grad_boost']\n",
    "    preprocessing = ['raw', 'pca', 'lda']\n",
    "    for model_name in classifiers:\n",
    "        model_options = {'method' : model_name }\n",
    "        for preprocessor in preprocessing:\n",
    "            preprocessing_options = {'method' : preprocessor }\n",
    "            metadata.append({'model' : model_options, \n",
    "                             'preprocess' : preprocessing_options })\n",
    "    pool = Pool(8)\n",
    "    results = pool.starmap_async(test, [(train_X, train_y, test_X, test_y, _) for _ in metadata])\n",
    "    results.get()\n",
    "    return results, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_X, train_y, test_X, test_y = [np.array(_) for _ in load_cifar()]\n",
    "    train_y = train_y.reshape(-1, 1)\n",
    "    test_y = test_y.reshape(-1, 1)\n",
    "    print(\"Before processing\", [_.shape for _ in [train_X, train_y, test_X, test_y]])\n",
    "    train_X, train_y, onehot = image_prep(train_X, train_y, onehot=False)\n",
    "#     test_y = onehot.transform(test_y)\n",
    "    results, meta_ = evaluate(train_X, train_y, test_X, test_y)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before processing [(50000, 3072), (50000, 1), (10000, 3072), (10000, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/.virtualenvs/smai/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/pratik/.virtualenvs/smai/lib/python3.6/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After processing [(50000, 3072), (50000, 1), (10000, 3072), (10000, 1)]\n",
      "Model SVM | Preprocessing raw started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/.virtualenvs/smai/lib/python3.6/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RBF | Preprocessing raw started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/.virtualenvs/smai/lib/python3.6/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logistic | Preprocessing raw started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/.virtualenvs/smai/lib/python3.6/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MLP | Preprocessing raw started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pratik/.virtualenvs/smai/lib/python3.6/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model CART | Preprocessing raw started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
